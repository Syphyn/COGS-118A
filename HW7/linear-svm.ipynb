{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM\n",
    "\n",
    "In this notebook you will write code to manually calculate SVM prediction and training via gradient descent on the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T08:16:58.697483Z",
     "start_time": "2020-02-11T08:16:58.241567Z"
    }
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Load the modified Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T08:16:58.707332Z",
     "start_time": "2020-02-11T08:16:58.699387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 375
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Iris dataset.\n",
    "iris = datasets.load_iris()     # Load Iris dataset.\n",
    "\n",
    "X = iris.data                   # The shape of X is (150, 4), which means\n",
    "                                # there are 150 data points, each data point\n",
    "                                # has 4 features.\n",
    "\n",
    "# Here for convenience, we divide the 3 kinds of flowers into 2 groups: \n",
    "#     Y = 0 (or False):  Setosa (original value 0) / Versicolor (original value 1)\n",
    "#     Y = 1 (or True):   Virginica (original value 2)\n",
    "\n",
    "# Thus we use (iris.target > 1.5) to divide the targets into 2 groups. \n",
    "# This line of code will assign:\n",
    "#    Y[i] = True  (which is equivalent to 1) if iris.target[k]  > 1.5 (Virginica)\n",
    "#    Y[i] = False (which is equivalent to 0) if iris.target[k] <= 1.5 (Setosa / Versicolor)\n",
    "\n",
    "Y = (iris.target > 1.5).astype(np.float) # The shape of Y is (150, 1), which means \n",
    "                                # there are 150 data points, each data point\n",
    "                                # has 1 target value. \n",
    "Y[Y==0] = -1\n",
    "\n",
    "plt.hist(Y,bins=[-1.5,-0.5,0.5,1.5]); # let's see the class distribution, looks like it's mostly Class -1!\n",
    "\n",
    "# get 100 in training set, 50 in test set. \n",
    "# Also let's stratify so that we force the training set to have the right class balance!\n",
    "# finally we're only going to use variables 1 & 3 for classification \n",
    "X_train, X_test, Y_train, Y_test = train_test_split( X[:,[3,1]], Y, test_size=50, \n",
    "                                                     stratify=Y, shuffle=True ,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T08:16:58.749772Z",
     "start_time": "2020-02-11T08:16:58.732332Z"
    }
   },
   "outputs": [],
   "source": [
    "def vis(X, Y, W=None, b=None):\n",
    "    indices_neg1 = (Y == -1).nonzero()[0]\n",
    "    indices_pos1 = (Y == 1).nonzero()[0]\n",
    "    plt.scatter(X[:,0][indices_neg1], X[:,1][indices_neg1], \n",
    "                c='blue', label='class -1')\n",
    "    plt.scatter(X[:,0][indices_pos1], X[:,1][indices_pos1], \n",
    "                c='red', label='class 1')\n",
    "    plt.legend()\n",
    "    plt.xlabel('$x_0$')\n",
    "    plt.ylabel('$x_1$')\n",
    "    \n",
    "    if W is not None:\n",
    "        # w0x0+w1x1+b=0 => x1=-w0x0/w1-b/w1\n",
    "        w0 = W[0]\n",
    "        w1 = W[1]\n",
    "        temp = -w1*np.array([X[:,1].min(), X[:,1].max()])/w0-b/w0\n",
    "        x0_min = max(temp.min(), X[:,0].min())\n",
    "        x0_max = min(temp.max(), X[:,1].max())\n",
    "        x0 = np.linspace(x0_min,x0_max,100)\n",
    "        x1 = -w0*x0/w1-b/w1\n",
    "        plt.plot(x0,x1,color='black')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T08:16:59.178336Z",
     "start_time": "2020-02-11T08:16:58.752051Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize training set.\n",
    "vis(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM Using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we would like to use the gradient descent to calculate the parameters $\\mathbf{w},b$ for a linear SVM model.\n",
    "If we have the loss function $\\mathcal{L}(\\mathbf{w},b)$, then a typical gradient descent algorithm contains the following steps:\n",
    "\n",
    "**Step 1**. Initialize the parameters $\\mathbf{w}$, $b$.\n",
    "\n",
    "for i = 1 to #iterations:\n",
    "\n",
    "- **Step 2**. Compute the partial derivatives $\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial \\mathbf{w}}$, $\\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial b}$.\n",
    "\n",
    "- **Step 3**. Update the parameters \n",
    "$$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\frac{\\partial \\mathcal{L}(\\mathbf{w}, b)}{\\partial \\mathbf{w}}, \\quad\\quad b \\leftarrow b - \\eta \\frac{\\partial \\mathcal{L}(\\mathbf{w},b)}{\\partial b}$$\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "Note that in the code, we use `W` and `b` to represent the weight vector $\\mathbf{w}$ and bias scalar $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: You may refer to HW4 Q6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T08:16:59.185219Z",
     "start_time": "2020-02-11T08:16:59.180080Z"
    }
   },
   "outputs": [],
   "source": [
    "# helper function: 1(a != b). It supports scalar, vector and matrix.\n",
    "def is_not_equal(a, b):\n",
    "    return np.array(a != b).astype(np.float32)\n",
    "\n",
    "# helper function: 1(z > 0). It supports scalar, vector and matrix.\n",
    "def is_positive(z):\n",
    "    return np.array(z > 0).astype(np.float32)\n",
    "    \n",
    "# Rectifier function: (z)_+ = max(0, z). It supports scalar, vector and matrix.\n",
    "def rectifier(z):\n",
    "    return np.clip(z, a_min=0, a_max=None)\n",
    "\n",
    "# The prediction step of a trained linear SVM classifier.\n",
    "def f_linear_svm(x, W, b):\n",
    "    # x should be a 2-dimensional vector, \n",
    "    # W should be a 2-dimensional vector,\n",
    "    # b should be a scalar.\n",
    "    # you should return a scalar which is -1 or 1. \n",
    "    ######### To be filled. #########\n",
    "    \n",
    "# Calculate error given feature vectors X and labels Y.\n",
    "def calc_error(X, Y, W, b):\n",
    "    e = 0\n",
    "    n = len(X)\n",
    "    for (xi, yi) in zip(X, Y):\n",
    "        e += is_not_equal(yi, f_linear_svm(xi, W, b))\n",
    "    e = 1.0 * e / n\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T08:16:59.207407Z",
     "start_time": "2020-02-11T08:16:59.186969Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gradient of L(W, b) with respect to W and b.\n",
    "def grad_L_W_b(X, Y, W, b, C):\n",
    "    ######### To be filled. #########\n",
    "    grad_W = ######### To be filled. #########  Hint: Use is_positive().\n",
    "    grad_b = ######### To be filled. #########\n",
    "    return grad_W, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T08:16:59.235690Z",
     "start_time": "2020-02-11T08:16:59.208510Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss L(W, b).\n",
    "def L_W_b(X, Y, W, b, C):\n",
    "    ######### To be filled. #########\n",
    "    return ######### To be filled. #########  Hint: Use rectifier()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T08:16:59.544986Z",
     "start_time": "2020-02-11T08:16:59.236832Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some settings.\n",
    "learning_rate = 0.00005\n",
    "iterations    = 10000\n",
    "losses = []\n",
    "\n",
    "# Gradient descent algorithm for linear SVM classifier.\n",
    "# Step 1. Initialize the parameters W, b.\n",
    "W = np.zeros(2) \n",
    "b = 0\n",
    "C = 1000\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Step 2. Compute the partial derivatives.\n",
    "    grad_W, grad_b = grad_L_W_b(X_train, Y_train, W, b, C) \n",
    "    # Step 3. Update the parameters.\n",
    "    W = W - learning_rate * grad_W\n",
    "    b = b - learning_rate * grad_b\n",
    "\n",
    "    # Track the training losses.\n",
    "    losses.append(L_W_b(X_train, Y_train, W, b, C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T08:16:59.877763Z",
     "start_time": "2020-02-11T08:16:59.546232Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show decision boundary, training error and test error.\n",
    "print('Decision boundary: {:.3f}x0+{:.3f}x1+{:.3f}=0'.format(W[0],W[1],b))\n",
    "vis(X_train, Y_train, W, b)\n",
    "print('Training error: {}'.format(calc_error(X_train, Y_train, W, b)))\n",
    "vis(X_test, Y_test, W, b)\n",
    "print('Test error: {}'.format(calc_error(X_test, Y_test, W, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T08:17:00.064400Z",
     "start_time": "2020-02-11T08:16:59.879107Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training loss curve.\n",
    "plt.title('Training loss curve')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(losses)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
