{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Scikit-Learn\n",
    "In this notebook you will find the best hyperparameters for an SVM using an RBF kernel operating on this data.\n",
    "\n",
    "The fitting and tuning will be done on a k-fold cross validation on the training data. Using the k-fold reserves the seperate test set for evaluating the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T21:55:27.954960Z",
     "start_time": "2020-02-14T21:55:26.272338Z"
    }
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load the modified Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T21:55:27.966007Z",
     "start_time": "2020-02-14T21:55:27.956813Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Iris dataset.\n",
    "iris = datasets.load_iris()     # Load Iris dataset.\n",
    "\n",
    "X = iris.data                   # The shape of X is (150, 4), which means\n",
    "                                # there are 150 data points, each data point\n",
    "                                # has 4 features.\n",
    "\n",
    "# Here for convenience, we divide the 3 kinds of flowers into 2 groups: \n",
    "#     Y = 0 (or False):  Setosa (original value 0) / Versicolor (original value 1)\n",
    "#     Y = 1 (or True):   Virginica (original value 2)\n",
    "\n",
    "# Thus we use (iris.target > 1.5) to divide the targets into 2 groups. \n",
    "# This line of code will assign:\n",
    "#    Y[i] = True  (which is equivalent to 1) if iris.target[k]  > 1.5 (Virginica)\n",
    "#    Y[i] = False (which is equivalent to 0) if iris.target[k] <= 1.5 (Setosa / Versicolor)\n",
    "\n",
    "Y = (iris.target > 1.5).astype(np.float) # The shape of Y is (150, 1), which means \n",
    "                                # there are 150 data points, each data point\n",
    "                                # has 1 target value. \n",
    "Y[Y==0] = -1\n",
    "\n",
    "plt.hist(Y,bins=[-1.5,-0.5,0.5,1.5]); # let's see the class distribution, looks like it's mostly Class -1!\n",
    "\n",
    "# get 100 in training set, 50 in test set. \n",
    "# Also let's stratify so that we force the training set to have the right class balance!\n",
    "# this time we will use all the variables, not jsut 3 & 1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size=50, \n",
    "                                                     stratify=Y, shuffle=True ,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with RBF Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T21:55:28.010137Z",
     "start_time": "2020-02-14T21:55:27.991755Z"
    }
   },
   "outputs": [],
   "source": [
    "# Draw the heatmap of training errors.\n",
    "def draw_heatmap(training_errors):\n",
    "    # training_errors: A NumPy array with the shape (len(C_list), len(gamma_list))\n",
    "    # gamma_list: List of gamma(s).\n",
    "    # C_list: List of C(s).\n",
    "    plt.figure(figsize = (5,4))\n",
    "    ax = sns.heatmap(training_errors, annot=True, fmt='.3f', \n",
    "                     xticklabels=gamma_list, yticklabels=C_list)\n",
    "    ax.collections[0].colorbar.set_label(\"error\")\n",
    "    ax.set(xlabel = '$\\gamma$', ylabel='$C$')\n",
    "    plt.title('Training error w.r.t $C$ and $\\gamma$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T21:55:28.063514Z",
     "start_time": "2020-02-14T21:55:28.011864Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "C_list = [1, 10, 100, 1000, 10000]\n",
    "gamma_list = [1e-6, 1e-5, 1e-4, 1e-3,1e-2]\n",
    "\n",
    "# An example of using draw_heatmap().\n",
    "#    errors = np.random.random((len(C_list), len(gamma_list)))\n",
    "#    draw_heatmap(errors, gamma_list, C_list)\n",
    "\n",
    "opt_e_training = 1.0   # Optimal training error.\n",
    "opt_classifier = None  # Optimal classifier.\n",
    "opt_C          = None  # Optimal C.\n",
    "opt_gamma      = None  # Optimal C.\n",
    "\n",
    "# Training errors\n",
    "training_errors = np.zeros((len(C_list), len(gamma_list)))\n",
    "\n",
    "for i, C in enumerate(C_list):\n",
    "    for j, gamma in enumerate(gamma_list):\n",
    "        # Create a SVM classifier with RBF kernel.\n",
    "        classifier = ######### PLACE YOUR CODE HERE #########\n",
    "        \n",
    "        # get the accuracy score from 5-fold cross-validation of the training data; see sklearn docs\n",
    "        e_training = cross_val_score( classifier, X_train, Y_train, cv=5)\n",
    "        # the above returns the accuracy on each fold, take the mean and inverse for misclassification error rate\n",
    "        e_training = 1 - np.mean( e_training ) \n",
    "        \n",
    "        training_errors[i,j] = e_training\n",
    "        \n",
    "        if e_training < opt_e_training:\n",
    "            opt_e_training = e_training\n",
    "            opt_classifier = classifier\n",
    "            opt_C          = C\n",
    "            opt_gamma      = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T21:55:28.597467Z",
     "start_time": "2020-02-14T21:55:28.065136Z"
    }
   },
   "outputs": [],
   "source": [
    "draw_heatmap(training_errors)#, gamma_list, C_list)\n",
    "opt_classifier.fit(X_train,Y_train)\n",
    "opt_test_score = 1. - opt_classifier.score(X_test,Y_test) # again turn accuracy into misclassification error\n",
    "print(f'Test set error for optimum classifier(C={opt_C},gamma={opt_gamma}): {opt_test_score:.3f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But wait, there's an easier way!\n",
    "You can do all the manual work of looping we did above using just 1 command in sklearn.  \n",
    "\n",
    "Below please write the correct function call to duplicate the above 5-fold cross validation of `C_list` and `gamma_list` parameter sets using the function [GridSearchCV() and its documentation found here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "classifier = svm.SVC(kernel='rbf')\n",
    "search_results =  ######### PLACE YOUR CODE HERE #########\n",
    "\n",
    "search_results.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's how we can extract all the same information from the search_results data structure\n",
    "\n",
    "# first get the parameters used in the order they were used\n",
    "results = pd.DataFrame( search_results.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score'] = 1 - search_results.cv_results_['mean_test_score']\n",
    "\n",
    "# turn a long table into a 2-D table with C being the 1st axis (y-axis) and gamma as the 2nd (x-axis)\n",
    "results = results.pivot('C','gamma','score')\n",
    "\n",
    "# and plot it\n",
    "# using the dataframe means we don't need the wrapper fct around sns.heatmap we used before\n",
    "sns.heatmap(results,annot=True, fmt='.3f') \n",
    "plt.title('Training error w.r.t $C$ and $\\gamma$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the best estimator and its params\n",
    "opt_classifier = search_results.best_estimator_\n",
    "opt_C = search_results.best_params_['C']\n",
    "opt_gamma = search_results.best_params_['gamma']\n",
    "\n",
    "# just a copy/paste of test set evaluation from before\n",
    "opt_classifier.fit(X_train,Y_train)\n",
    "opt_test_score = 1 - opt_classifier.score(X_test,Y_test) # again turn accuracy into misclassification error\n",
    "print(f'Test set error for optimum classifier(C={opt_C},gamma={opt_gamma}): {opt_test_score:.3f}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
